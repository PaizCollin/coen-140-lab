{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paizc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "import random\n",
    "import faiss\n",
    "import time\n",
    "from pynndescent import NNDescent\n",
    "import elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "data = pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Initialising the snowball stemmer\n",
    "sno = nltk.stem.SnowballStemmer('english') \n",
    "\n",
    "# Function to clean the word of any html-tags\n",
    "def cleanhtml(sentence): \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "\n",
    "# Function to clean the word of any punctuation or special characters\n",
    "def cleanpunc(sentence): \n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned\n",
    "\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "\n",
    "# Store words from +ve reviews \n",
    "all_positive_words=[]\n",
    "\n",
    "# Store words from -ve reviews\n",
    "all_negative_words=[] \n",
    "s=''\n",
    "\n",
    "final_string=[]\n",
    "all_positive_words=[] \n",
    "all_negative_words=[] \n",
    "s=''\n",
    "for sent in data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "\n",
    "    # Remove HTMl tags\n",
    "    sent=cleanhtml(sent) \n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words    \n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() \n",
    "final_bow_count = count_vect.fit_transform(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bow_np = StandardScaler(with_mean=False).fit_transform(final_bow_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_bow_np\n",
    "\n",
    "X_train =  final_bow_np[:math.ceil(len(data)*.9)] \n",
    "X_test = final_bow_np[math.ceil(len(data)*.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 16482)\n",
      "(2000, 16482)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot assign slice from input of different size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\paizc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pynndescent\\pynndescent_.py:804\u001b[0m, in \u001b[0;36mNNDescent.__init__\u001b[1;34m(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[39mprint\u001b[39m(ts(), \u001b[39m\"\u001b[39m\u001b[39mBuilding RP forest with\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(n_trees), \u001b[39m\"\u001b[39m\u001b[39mtrees\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    794\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rp_forest \u001b[39m=\u001b[39m make_forest(\n\u001b[0;32m    795\u001b[0m         data,\n\u001b[0;32m    796\u001b[0m         n_neighbors,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    802\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_angular_trees,\n\u001b[0;32m    803\u001b[0m     )\n\u001b[1;32m--> 804\u001b[0m     leaf_array \u001b[39m=\u001b[39m rptree_leaf_array(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rp_forest)\n\u001b[0;32m    805\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rp_forest \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paizc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pynndescent\\rp_trees.py:1097\u001b[0m, in \u001b[0;36mrptree_leaf_array\u001b[1;34m(rp_forest)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrptree_leaf_array\u001b[39m(rp_forest):\n\u001b[0;32m   1096\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(rp_forest) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1097\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mvstack(rptree_leaf_array_parallel(rp_forest))\n\u001b[0;32m   1098\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\paizc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pynndescent\\rp_trees.py:1089\u001b[0m, in \u001b[0;36mrptree_leaf_array_parallel\u001b[1;34m(rp_forest)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrptree_leaf_array_parallel\u001b[39m(rp_forest):\n\u001b[1;32m-> 1089\u001b[0m     result \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mParallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, require\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msharedmem\u001b[39;49m\u001b[39m\"\u001b[39;49m)(\n\u001b[0;32m   1090\u001b[0m         joblib\u001b[39m.\u001b[39;49mdelayed(get_leaves_from_tree)(rp_tree) \u001b[39mfor\u001b[39;49;00m rp_tree \u001b[39min\u001b[39;49;00m rp_forest\n\u001b[0;32m   1091\u001b[0m     )\n\u001b[0;32m   1092\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\paizc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\paizc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\paizc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\paizc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    123\u001b[0m job, i, func, args, kwds \u001b[39m=\u001b[39m task\n\u001b[0;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     result \u001b[39m=\u001b[39m (\u001b[39mTrue\u001b[39;00m, func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m wrap_exception \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[1;32mc:\\Users\\paizc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:620\u001b[0m, in \u001b[0;36mSafeFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    619\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    621\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    622\u001b[0m         \u001b[39m# We capture the KeyboardInterrupt and reraise it as\u001b[39;00m\n\u001b[0;32m    623\u001b[0m         \u001b[39m# something different, as multiprocessing does not\u001b[39;00m\n\u001b[0;32m    624\u001b[0m         \u001b[39m# interrupt processing for a KeyboardInterrupt\u001b[39;00m\n\u001b[0;32m    625\u001b[0m         \u001b[39mraise\u001b[39;00m WorkerInterrupt() \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paizc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[1;32mc:\\Users\\paizc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "\u001b[1;31mValueError\u001b[0m: cannot assign slice from input of different size"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 50\n",
    "\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "yaya = NNDescent(X_train_dense, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = np.array(distances)\n",
    "neighbors = np.array(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(pred, gt):\n",
    "    \"\"\"Compute the recall of the pred neighborhood vs. the gt neighborhood\"\"\"\n",
    "    if pred.shape[0] == 1:\n",
    "        return len(set(pred).intersection(set(gt)))/len(gt)\n",
    "    assert pred.shape[0] == gt.shape[0]\n",
    "    return np.array([len(set(pred[i]).intersection(set(gt[i])))/len(gt[i]) for i in range(len(pred))]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall for a random assignment of neighbors\n",
    "recall(np.random.randint(len(neighbors), size=(len(neighbors), k)), neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall if the last 3 neighbors were missed for each sample\n",
    "recall(neighbors[:, range(k-3)], neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall if you found all the correct neighbors\n",
    "recall(neighbors, neighbors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
