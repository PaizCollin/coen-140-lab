{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paizc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "import random\n",
    "import faiss\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "data = pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Initialising the snowball stemmer\n",
    "sno = nltk.stem.SnowballStemmer('english') \n",
    "\n",
    "# Function to clean the word of any html-tags\n",
    "def cleanhtml(sentence): \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "\n",
    "# Function to clean the word of any punctuation or special characters\n",
    "def cleanpunc(sentence): \n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned\n",
    "\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "\n",
    "# Store words from +ve reviews \n",
    "all_positive_words=[]\n",
    "\n",
    "# Store words from -ve reviews\n",
    "all_negative_words=[] \n",
    "s=''\n",
    "\n",
    "final_string=[]\n",
    "all_positive_words=[] \n",
    "all_negative_words=[] \n",
    "s=''\n",
    "for sent in data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "\n",
    "    # Remove HTMl tags\n",
    "    sent=cleanhtml(sent) \n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words    \n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() \n",
    "final_bow_count = count_vect.fit_transform(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bow_np = StandardScaler(with_mean=False).fit_transform(final_bow_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_bow_np\n",
    "\n",
    "X_train =  final_bow_np[:math.ceil(len(data)*.9)] \n",
    "X_test = final_bow_np[math.ceil(len(data)*.9):]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.17 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 50\n",
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='cosine').fit(X_train)\n",
    "gt_distances, gt_neighbors = nbrs.kneighbors(X_test, k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 54.7 s\n",
      "Wall time: 6.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 50\n",
    "\n",
    "n_dimensions = X_train.shape[1]\n",
    "\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "index = faiss.IndexFlatL2(n_dimensions) \n",
    "index.train(X_train_dense)\n",
    "\n",
    "index.add(X_train_dense)\n",
    "\n",
    "distances, neighbors = index.search(X_test_dense, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = np.array(distances)\n",
    "neighbors = np.array(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(pred, gt):\n",
    "    \"\"\"Compute the recall of the pred neighborhood vs. the gt neighborhood\"\"\"\n",
    "    if pred.shape[0] == 1:\n",
    "        return len(set(pred).intersection(set(gt)))/len(gt)\n",
    "    assert pred.shape[0] == gt.shape[0]\n",
    "    return np.array([len(set(pred[i]).intersection(set(gt[i])))/len(gt[i]) for i in range(len(pred))]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00276"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall for a random assignment of neighbors\n",
    "recall(np.random.randint(len(neighbors), size=(len(neighbors), k)), gt_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19443000000000002"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall if the last 3 neighbors were missed for each sample\n",
    "recall(neighbors[:, range(k-3)], gt_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19671000000000002"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall if you found all the correct neighbors\n",
    "recall(neighbors, gt_neighbors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
