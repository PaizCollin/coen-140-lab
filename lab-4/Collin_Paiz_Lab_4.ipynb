{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "hd = fetch_california_housing()\n",
    "housing = pd.DataFrame(hd.data, columns=hd.feature_names)\n",
    "housing['target'] = hd.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(t, X, y):\n",
    "    \"\"\"Compute the current error and gradient.\"\"\"\n",
    "    # Hypothesis/estimate values for y\n",
    "    y_estimate = X.dot(t).flatten()\n",
    "    # Loss - the difference between the estimated and actual values of y\n",
    "    loss = y.flatten() - y_estimate\n",
    "    m = float(len(X))\n",
    "    # Compute gradient\n",
    "    grad = -(1.0 / m) * loss.dot(X)\n",
    "    # Cost function value\n",
    "    cost = (0.5 / m) * np.sum(np.power(loss, 2))\n",
    "    return grad, cost\n",
    "\n",
    "def compute_cost(t, X, y):\n",
    "    \"\"\"Compute the current error/cost.\"\"\"\n",
    "    y_estimate = X.dot(t).flatten()\n",
    "    loss = y.flatten() - y_estimate\n",
    "    m = float(len(X))\n",
    "    return (0.5 / m) * np.sum(np.power(loss, 2))\n",
    "\n",
    "def gradient_descent(X, y, alpha=0.5, tolerance=1e-5, maxit=1e+6, nulbias=False, n=2):\n",
    "    \"\"\"Finds the best line fit for predicting y given x.\n",
    "       Keep track of and also return tested models, gradients, and errors \n",
    "       along the optimization path.\n",
    "    \"\"\"\n",
    "    # add intercept term to X -- accounts for the bias -- and normalize X's\n",
    "    X_norm = np.apply_along_axis(lambda x: x/x.max(), 0, X)\n",
    "    X_norm = np.hstack((np.ones((X_norm.shape[0], 1)), X_norm))\n",
    "    # start with a random (or zeros) theta vector\n",
    "    t = np.random.randn(X_norm.shape[1])\n",
    "    if nulbias:\n",
    "        t[0] = 0\n",
    "    # perform gradient descent\n",
    "    it = 0\n",
    "    models = []\n",
    "    grads = []\n",
    "    errors = []\n",
    "    while it < maxit:\n",
    "        grad, error = gradient(t, X_norm, y)\n",
    "        models.append(t)\n",
    "        grads.append(grad)\n",
    "        errors.append(error)\n",
    "        new_t = t - alpha * grad\n",
    "        if nulbias:\n",
    "            new_t[0] = 0\n",
    "        # check whether we should stop\n",
    "        if np.sum(abs(new_t - t)) < tolerance:\n",
    "            break\n",
    "        # update theta\n",
    "        t = new_t\n",
    "        it += 1\n",
    "    if it == maxit:\n",
    "        print(\"Warning: reached maximum number of iterations without convergence.\")\n",
    "    return X_norm, t, models, grads, errors\n",
    "\n",
    "def plotmodel(x, y, t, start_at_zero=False):\n",
    "    \"\"\"Plot the line of a given model.\"\"\"\n",
    "    if t is not None:\n",
    "        if start_at_zero:\n",
    "            x = np.append([0], x)\n",
    "            y = np.append([0], y)\n",
    "        plt.plot(x, t[0] + x/x.max() * t[1], c='g', label='Model')\n",
    "#         equivalent to:\n",
    "#         X = np.vstack((np.ones_like(x), x/x.max())).T\n",
    "#         plt.plot(x, X.dot(t), c='g', label='Model')\n",
    "    plt.scatter(x, y, c='b', label='Data')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('MedInc')\n",
    "    plt.ylabel('Median House Price (x$100,000)')\n",
    "    if start_at_zero:\n",
    "        plt.ylim(ymin=0)\n",
    "        plt.xlim(xmin=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n",
      "[0.6536319715991409, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6536319715991409, 0, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = housing.to_numpy()\n",
    "rmse_values = []\n",
    "best = [1000, 0, 0]\n",
    "\n",
    "for i in range(0,7):\n",
    "    x1 = data[:,i]\n",
    "    for j in range(i+1, 8):\n",
    "        x2 = data[:,j]\n",
    "        X = np.column_stack((x1, x2))\n",
    "        y = data[:,8]\n",
    "        X_norm, t, models, grads, errors = gradient_descent(X, y, alpha=0.5, tolerance=1e-5, maxit=1e+6, nulbias=False, n=2)\n",
    "        # print(\"# iterations: \", len(models))\n",
    "        # print(\"first model: \", models[0])\n",
    "        # print(\"best model: \", t)\n",
    "\n",
    "        nits = len(models)\n",
    "        ts = [0, nits//4, nits//2, 3*(nits//4), nits-1] # quartles\n",
    "        # for k in ts:\n",
    "        #     print(\"Iteration: \", k+1)\n",
    "        #     print(\"Gradient: \", grads[k])\n",
    "        #     print(\"Error: \", errors[k])\n",
    "\n",
    "        # calculate predicted values of y for each model\n",
    "        y_preds = [X_norm.dot(model).flatten() for model in models]\n",
    "\n",
    "        # calculate RMSE for each model\n",
    "        n = len(y)\n",
    "        for y_pred in y_preds:\n",
    "            rmse = (1/n * np.sum(np.power(y - y_pred, 2)))\n",
    "            rmse_values.append([rmse, i, j])\n",
    "\n",
    "        besthere = min(rmse_values, key=lambda x: x[0])\n",
    "        print(besthere)\n",
    "\n",
    "    best = min([best, besthere], key=lambda x: x[0])\n",
    "\n",
    "best\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
